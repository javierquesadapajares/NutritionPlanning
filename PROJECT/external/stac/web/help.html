
<div class="panel-group" id="accordion" role="tablist">
	<!--Formato del fichero de datos-->
	<div class="panel panel-default">
		<div class="panel-heading" role="tab">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#file_info">1. Data file format</a>
			</h4>
		</div>
		<div id="file_info" class="panel-collapse collapse" role="tabpanel">
			<div class="panel-body">
				<p>The data file must be in csv ("comma separated values") format. That is, a plain text file in which each row represents an element of the data set, with its attributes separated by commas. Lets take the following image as an example:</p>
				<img src="img/fichero.png" class="img-responsive" alt="Formato fichero" width="130px;" height="290px;">
				<br>
				<p>It consists of the following elements:</p>
				<ul>
					<li>First row: datasets (any word would be valid before the first comma), algorithmName1, algorithmName2, ... , algorithmNameN</li>
					<li>Next rows: datasetName, algorithmResult1, algorithmResult2, ... , algorithmResultN</li>
				</ul>
				<br>
				<p>Examples:</p>
				<a href="./samples/data_sample1.csv" download="data_2_algorithm.csv">Data sample with 2 algorithms</a><br>
				<a href="./samples/data_sample2.csv" download="data_4_algorithm.csv">Data sample with 4 algorithms</a>
			</div>
		</div>
	</div>
	<!--Info sobre maximización/minimización-->
	<div class="panel panel-default">
		<div class="panel-heading" role="tab">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#meaning_info">2. Data file meaning</a>
			</h4>
		</div>
		<div id="meaning_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p>It is assumed in STAC that the lower the result of an algorithm on a problem, the better the performance of such algorithm. Let us consider the following example:</p>
				<img src="img/fichero.png" class="img-responsive" alt="Meaning fichero" width="130px;" height="290px;">
				<br>
				<p>It consists of the following elements:</p>
				<ul>
					<li>Here, on problem 4 (5th row) algorithm A performs better than algorithm B (45 vs 48).</li>
					<li>On the contrary, on problem 3 (4th row) algorithm A performs worse than algorithm B (64 vs 62).</li>
				</ul>
				<br>
				<p>If your datafile does not meet this criterion (i.e., in your case the higher the value, the better the algorithm performance), there are a number of simple transformations you can apply to your data.
					For example, just change the sign of each number in the file. In the previous example, the data file should look like:
				</p>
				<img src="img/ficheroNegativo.png" class="img-responsive" alt="Meaning fichero" width="130px;" height="290px;">

			</div>
		</div>
	</div>
	<!--Conceptos básicos de la web-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#basic_info">3. Basic concepts</a>
			</h4>
		</div>
		<div id="basic_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>Hypothesis testing:</b> Statistical problem which, in regard to a data sample, assumes that an initial hypothesis \({H_0}\) is true and check if the evidence against it is sufficiently strong in favor of an alternative hypothesis \({H_1}\). <a href="http://en.wikipedia.org/wiki/Statistical_hypothesis_testing" target="_blank">Learn more.</a></p>
				<p><b>Statistic:</b> Quantitative measure calculated with the tests. This quantitative measure provides the feasibility of the null hypothesis. Each test has its characteristic way of obtaining this value and the statistics follow a particular probability distribution (such as normal distribution, etc.).</p>
				<p><b>Significance level (\(\alpha\)):</b> Probability of rejecting a null hypothesis when it is true. Also known as confidence level or Type I error (false positive). <a href="http://en.wikipedia.org/wiki/Statistical_significance" target="_blank">Learn more.</a></p>
				<p><b>Power (\(1-\beta\))</b>: Probability of rejecting a null hypothesis when it is false. As the power increases, the chances of a Type II error (false negative or \(\beta\)) decrease. <a href="http://en.wikipedia.org/wiki/Statistical_power" target="_blank">Learn more.</a></p>
				<p><b>p-value:</b> Probability of obtaining a value of the statistic at least as extreme as the obtained in the test.  <a href="http://en.wikipedia.org/wiki/P-value" target="_blank">Learn more.</a></p>
				<p><b>Rejection of \(H_0\):</b> When the p-value is less than the significance level \({\alpha}\), the test is considered statistically significant and the null hypothesis is, therefore, rejected. <a href="http://en.wikipedia.org/wiki/Statistical_hypothesis_testing" target="_blank">Learn more.</a></p>
			</div>
		</div>
	</div>
	<!--Paramtric_vs_Non-parametric-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#parametric_info">4. Parametric vs Non-parametric</a>
			</h4>
		</div>
		<div id="parametric_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>Parametric test</b>: Statistical test that makes assumptions about the parameters of the population distribution of the data. These assumptions can be summarize as:</p>
				<ul>
				    <li>Normality: the distribution of the data follows a gaussian distribution. <a href="http://en.wikipedia.org/wiki/Normal_distribution" target="_blank">Learn more.</a></li>
				    <li>Homoscedasticity: the distributions of different groups are equal. <a href="http://en.wikipedia.org/wiki/Homoscedasticity" target="_blank">Learn more.</a></li>
				    <li>Independence: the values of a group are independent between them. <a href="http://en.wikipedia.org/wiki/Homoscedasticity" target="_blank">Learn more.</a></li>
				</ul>
				<p><b>Non-parametric test</b>: Statistical test that does not make assumptions about the parameters of the population distribution of the data. Less powerful than its parametric counterpart.</p>
			</div>
		</div>
	</div>
	<!--Multitest-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#multiple_info">5. Multiple comparisons</a>
			</h4>
		</div>
		<div id="multiple_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p>A two-step procedure is used to compare more than two groups of samples (algorithms):</p>
				<ul>
				    <li>Reject the null hypothesis that all the groups behave similarly, usually calculating a ranking of the groups. It is only necessary that two groups are different to reject the null hypothesis.</li>
				    <li>A post-hoc test that compare the groups in pairs to test if the difference between each pair is statistically significant. This procedure controls the familywise error rate, the increase in the Type I error due to the use of multiple related tests. <a href="http://en.wikipedia.org/wiki/Familywise_error_rate" target="_blank">Learn more.</a></li>
				</ul>
				<p>Two types of post-hoc tests can be distinguished:</p>
				<ul>
					<li>Control method: The first algorithm returned from the ranking test is compared with the others. There are \({K-1}\) comparisons.</li>
					<li>Multiple comparison (multitest): Compares all the algorithms between themselves. The number of comparisons would be \(m = {K*(K-1) \over 2}\).</li>
				</ul>
			</div>
		</div>
	</div>
	<!--Normalidad-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#normality_info">6. Normality tests</a>
			</h4>
		</div>
		<div id="normality_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>Shapiro-Wilk:</b> Tests the null hypothesis that the samples come from a normally distributed population. This is considered as one of the most powerful tests, especially for samples of less than 30 elements.</p>
				<p><b>D'Agostino-Pearson:</b> Tests the null hypothesis that the samples come from normally distributed population. The test combines the asymmetry coefficient (to what extent the normal is symmetric or of coefficient 0) and the coefficient of Kurtosis (degree of amplitude, usually of coefficient 0) in order to obtain an statistic and p-value. It's less powerful than Shapiro-Wilk.</p>
				<p><b>Kolmogorov-Smirnov:</b> Performs a test of goodness of fit, to determine if the data follows a normal distribution. It assumes, as H0, that the distribution obtained from the observed data is identical to the normal distribution. This is the least powerful offering the worst results off the three.</p>
			</div>
		</div>
	</div>
	<!--Test de homocedasticidad-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#homocedasticity_info">7.  Homoscedasticity test</a>
			</h4>
		</div>
		<div id="homocedasticity_info" class="panel-collapse collapse">
			<div class="panel-body">
				 <p><b>Levene:</b> Tests the null hypothesis that all the input populations come from populations with equal variances. If the p-value obtained from Levene's test is less than the significance level, the hypothesis is rejected. Parametric tests such as Anova or T-test, assume homoscedasticity in the populations.</p>
			</div>
		</div>
	</div>
	<!--Test T-test-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#parametric_two_info">8. Parametric two groups tests</a>
			</h4>
		</div>
		<div id="parametric_two_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>t-test</b>: Tests the null hypothesis that 2 related or repeated samples have identical mean values. The test checks if the mean score differs significantly between the samples.  <a href="http://en.wikipedia.org/wiki/Student%27s_t-test" target="_blank">Learn more.</a></p>
				<p>It can be applied to:</p>
				<ul>
				    <li>Paired data: the data values of each group are related.</li>
				    <li>Unpaired data: the data values of each group are independent.</li>
				</ul>
			</div>
		</div>
	</div>
	<!--Test de Anova-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#parametric_multi_info">9. Parametric multiple groups tests</a>
			</h4>
		</div>
		<div id="parametric_multi_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>ANOVA:</b> Tests the null hypothesis that the means of the results of two or more groups are the same. For this, the test analyzes the variation between samples as well as their inner variation with the variance. The statistic of the ANOVA test, is estimated by the f-distribution.</p>
				<p><b>Bonferroni:</b> Once evidence of the existence of significant differences between the means of the algorithms is achieved, thanks to the variance analysis of ANOVA, it is possible to proceed with the POST-HOC test of Bonferroni in order to determine the discrepancies between all the samples, comparing the means of all the algorithms. Each p-value associated with the hypothesis \({H_i}\) is compared taking an \({\alpha}\) adjusted to all the comparisons: \(p_i &lt; {\alpha \over m}\), where \({K}\) is the number of algorithms and m is the number of comparisons: \(m = {K*(K-1) \over 2}\)</p>
			</div>
		</div>
	</div>
	<!--Test de Wilcoxon-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#nonparametric_two_info">10. Non-parametric two groups tests</a>
			</h4>
		</div>
		<div id="nonparametric_two_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>Wilcoxon</b>: Tests the null hypothesis that two related paired samples come from the same distribution. It assumes that the differences between samples are symmetrical with respect to the median.  <a href="http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test" target="_blank">Learn more.</a></p>
				<p><b>Binomial Sign</b>: Tests the null hypothesis that two related paired samples come from the same distribution. It does not assume symmetry but is less powerfull than Wilcoxon. <a href="http://en.wikipedia.org/wiki/Sign_test" target="_blank">Learn more.</a></p>
				<p><b>Mann-Whitney-U</b>: Tests the null hypothesis that two non-paired samples come from the same distribution. <a href="http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test" target="_blank">Learn more.</a></p>
			</div>
		</div>
	</div>
	<!--Tests de ranking disponibles.-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#nonparametric_multi_info">11. Non-parametric multiple groups tests</a>
			</h4>
		</div>
		<div id="nonparametric_multi_info" class="panel-collapse collapse">
			<div class="panel-body">
				<p><b>Friedman:</b> This test makes comparisons and assigns rankings to each data set. The statistic follows a chis-quared distribution with \({K-1}\) degrees of freedom, being \({K}\) the number of related variables (or number of algorithms).</p>
				<p><b>Friedman's aligned ranks:</b> It makes comparisons an assigns rankings considering all the data sets. It is usually employed when the number of algorithms in the comparison is low.</p>
				<p><b>Quade:</b> Similar to ImanDavenport, only that it takes into account that some problems are more difficult or that the results obtained from different algorithms present higher discrepancies (weighting).</p>
			</div>
		</div>
	</div>
	<!--Tests Post-Hoc disponibles.-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#post-hoc">12. Non-parametric post-hoc tests</a>
			</h4>
		</div>
		<div id="post-hoc" class="panel-collapse collapse">
			<div class="panel-body">
				<p><strong>Bonferroni-Dunn: </strong>This test rejects the null hypothesis if: \(p-value &lt; {\alpha \over (K-1)}\), where \({K}\) is the number of algorithms in the sample. It is a very conservative test and many differences can not be detected (the worst power).</p>
				<p><strong>Holm: </strong>It compares each \({p_i}\) (starting from the most significant or the lowest) with: \({\alpha \over (K-i)}\), where \({i \in [1,K-1]}\). If the hypothesis is rejected the test continues the comparisons. When an hypothesis is accepted, all the other hypothesis are accepted as well. It is better (more power) than Bonferroni-Dunn test, because it controls the FWER (familywise error rate), which is the probability of committing one or more type I errors among all hypothesis.</p>
				<p><strong>Hochberg: </strong>It compares in the opposite direction to Holm. As soon as an acceptable hypothesis is found, all the other hypothesis are accepted. It is better (more power) than Holm test, but the differences between them are small in practice.</p>
				<p><strong>Finner: </strong>Finner's test is similar to Holm's but each p-value associated with the hypothesis \({H_i}\) is compared with: \(p_i \leq {1-(1-\alpha)^{\frac{(K-1)}{i}}}\), where \({i \in [1,K-1]}\). It is more powerful than Bonferroni-Dunn, Holm, Hochberg and Li (only in some cases).</p>
				<p><strong>Li: </strong>This test rejects all the hypothesis if the least significant p-value is less than \({\alpha}\) (significance level). Else, the test accepts the hypothesis and rejects any remaining hypothesis whose p-value is less than: \(value = \frac{(1-p-value_{K-1})}{(1-\alpha)\alpha}\). The author states that the power of this test is Highly influenced by the p-value (greater power when the p-value is less than 0.5).</p>
				<p><strong>Multitests: </strong>Just like the others but with \({m}\) comparisons instead of \({K-1}\), where \(m = {K*(K-1) \over 2}\).</p>
				<p><strong>Shaffer: </strong>This test is like Holm's but each p-value associated with the hypothesis \({H_i}\) is compared as \(p_i \leq {\alpha \over t_i}\), where \({t_i}\) is the maximum number of possible hypothesis assuming that the previous \({(j-1)}\) hypothesis have been rejected.</p>
			</div>
		</div>
	</div>
	<!--Más información acerca de la web-->
	<div class="panel panel-default">
		<div class="panel-heading">
			<h4 class="panel-title">
				<a data-toggle="collapse" data-parent="#accordion" href="#about">13. About the site</a>
			</h4>
		</div>
		<div id="about" class="panel-collapse collapse">
			<div class="panel-body">
				<p>STAC</p>
				<p>Web plataform for the statistical comparison of algorithms</p>
				<p>Version 1.0</p>
				<br/>
				<p>Final Degree Project developed by by: Adrián Canosa Mouzo. Universidad de Santiago de Compostela - Escuela Técnica Superior de Ingeniería.</p>
				<p>STAC logo was designed by <a href="https://github.com/angelpinheiro">Ángel Piñeiro Souto</a></p>
			</div>
		</div>
	</div>
</div>


<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		jax: ["input/TeX","output/HTML-CSS"],
		displayAlign: "left"
	});
</script>



